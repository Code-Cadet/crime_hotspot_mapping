{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1199d6d0",
   "metadata": {},
   "source": [
    "\n",
    "This notebook conducts a comprehensive evaluation of our tuned models on the full dataset, selects the best model for deployment, and generates detailed performance reports and visualizations.\n",
    "\n",
    "## Table of Contents\n",
    "1. Model Loading and Setup\n",
    "2. Final Training on Full Dataset\n",
    "3. Comprehensive Model Evaluation\n",
    "4. Performance Visualization\n",
    "5. Model Selection and Justification\n",
    "6. Model Persistence and Reporting\n",
    "\n",
    "## Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9119a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Model persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Evaluation metrics and tools\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Base models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('default')  # Use default matplotlib style\n",
    "sns.set_theme()  # Set seaborn defaults\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Create directories for outputs\n",
    "figures_dir = Path('figures')\n",
    "models_dir = Path('models')\n",
    "results_dir = Path('results')\n",
    "\n",
    "for dir_path in [figures_dir, models_dir, results_dir]:\n",
    "    dir_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090a81e",
   "metadata": {},
   "source": [
    "\n",
    "Load the full preprocessed dataset and the tuned models from our previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45e3b461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "Training set shape: (320000, 11)\n",
      "Test set shape: (80000, 11)\n",
      "\n",
      "Feature Summary:\n",
      "       risk_score_rolling_std  dist_from_center   time_window              y  \\\n",
      "count           400000.000000     400000.000000  400000.00000  400000.000000   \n",
      "mean                 0.044015         38.928369      82.83400      50.135355   \n",
      "std                  0.029310         14.302932      48.11287      29.287526   \n",
      "min                  0.000000          0.000000       0.00000       0.000000   \n",
      "25%                  0.022878         29.068884      41.00000      25.000000   \n",
      "50%                  0.038877         40.447497      83.00000      50.000000   \n",
      "75%                  0.059539         49.497475     124.25000      76.000000   \n",
      "max                  0.308365         70.710678     166.00000      99.000000   \n",
      "\n",
      "               hour  offender_target_ratio        grid_id     num_nearby  \\\n",
      "count  400000.00000          400000.000000  400000.000000  400000.000000   \n",
      "mean       11.43600               0.414622      50.827537       0.634950   \n",
      "std         6.90956               0.140433      29.069583       0.801935   \n",
      "min         0.00000               0.164350       0.000000       0.000000   \n",
      "25%         5.00000               0.308837      26.000000       0.000000   \n",
      "50%        11.00000               0.391969      51.000000       0.000000   \n",
      "75%        17.00000               0.494714      76.000000       1.000000   \n",
      "max        23.00000               0.780427      99.000000       6.000000   \n",
      "\n",
      "       dist_to_high_risk    day_of_week     event_rate  \n",
      "count      400000.000000  400000.000000  400000.000000  \n",
      "mean            0.634428       2.976000       0.952483  \n",
      "std             0.728735       1.989833       0.187303  \n",
      "min             0.000000       0.000000       0.001931  \n",
      "25%             0.000000       1.000000       1.000000  \n",
      "50%             0.000000       3.000000       1.000000  \n",
      "75%             1.000000       5.000000       1.000000  \n",
      "max             2.828427       6.000000       1.000000  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/processed/engineered_features.csv')\n",
    "\n",
    "# Selected features from previous analysis\n",
    "selected_features = [\n",
    "    'risk_score_rolling_std',  # Primary predictors\n",
    "    'dist_from_center',\n",
    "    'time_window',\n",
    "    'y',\n",
    "    'hour',\n",
    "    'offender_target_ratio',   # Secondary predictors\n",
    "    'grid_id',\n",
    "    'num_nearby',\n",
    "    'dist_to_high_risk',       # Supporting predictors\n",
    "    'day_of_week',\n",
    "    'event_rate'\n",
    "]\n",
    "\n",
    "# Prepare features and target\n",
    "X = data[selected_features]\n",
    "y = data['risk_score']\n",
    "\n",
    "# Clean the data by replacing infinite values with NaN and then filling them\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# For each column, fill NaN with the median of that column\n",
    "for column in X.columns:\n",
    "    X[column] = X[column].fillna(X[column].median())\n",
    "\n",
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create preprocessor\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(\"\\nFeature Summary:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a75d77",
   "metadata": {},
   "source": [
    "\n",
    "Create models with the best hyperparameters found during tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23498dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = {\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt'\n",
    "}\n",
    "\n",
    "gb_best_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'max_features': 'sqrt'\n",
    "}\n",
    "\n",
    "# Initialize models with best parameters\n",
    "rf_model = RandomForestRegressor(**rf_best_params, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(**gb_best_params, random_state=42)\n",
    "baseline_model = LinearRegression()\n",
    "\n",
    "# Create pipelines\n",
    "def create_pipeline(model):\n",
    "    return Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "rf_pipeline = create_pipeline(rf_model)\n",
    "gb_pipeline = create_pipeline(gb_model)\n",
    "baseline_pipeline = create_pipeline(baseline_model)\n",
    "\n",
    "# Create voting ensemble\n",
    "voting_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', VotingRegressor([\n",
    "        ('rf', rf_model),\n",
    "        ('gb', gb_model)\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64d9d1",
   "metadata": {},
   "source": [
    "\n",
    "Define helper functions for model evaluation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eb416d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation function.\"\"\"\n",
    "    # Train the model\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "        'train_mae': mean_absolute_error(y_train, train_pred),\n",
    "        'test_mae': mean_absolute_error(y_test, test_pred),\n",
    "        'train_r2': r2_score(y_train, train_pred),\n",
    "        'test_r2': r2_score(y_test, test_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return metrics, train_pred, test_pred\n",
    "\n",
    "def plot_feature_importance(model, feature_names, title=\"Feature Importance\"):\n",
    "    \"\"\"Plot feature importance for tree-based models.\"\"\"\n",
    "    if hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "        importances = model.named_steps['regressor'].feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]  # Sort in descending order\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(indices)), importances[indices])\n",
    "        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Relative Importance')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, title=\"Residual Plot\"):\n",
    "    \"\"\"Create residual plot with distribution.\"\"\"\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Scatter plot of residuals\n",
    "    ax1.scatter(y_pred, residuals, alpha=0.5)\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    ax1.set_xlabel('Predicted Values')\n",
    "    ax1.set_ylabel('Residuals')\n",
    "    ax1.set_title(f'{title} - Scatter')\n",
    "    \n",
    "    # Distribution of residuals\n",
    "    sns.histplot(residuals, kde=True, ax=ax2)\n",
    "    ax2.axvline(x=0, color='r', linestyle='--')\n",
    "    ax2.set_xlabel('Residual Value')\n",
    "    ax2.set_title(f'{title} - Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def temporal_performance(model, X, y, time_column):\n",
    "    \"\"\"Calculate performance metrics across time periods.\"\"\"\n",
    "    predictions = model.predict(X)\n",
    "    time_periods = X[time_column].unique()\n",
    "    temporal_metrics = []\n",
    "    \n",
    "    for period in sorted(time_periods):\n",
    "        mask = X[time_column] == period\n",
    "        period_pred = predictions[mask]\n",
    "        period_true = y[mask]\n",
    "        \n",
    "        metrics = {\n",
    "            'period': period,\n",
    "            'rmse': np.sqrt(mean_squared_error(period_true, period_pred)),\n",
    "            'r2': r2_score(period_true, period_pred)\n",
    "        }\n",
    "        temporal_metrics.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(temporal_metrics)\n",
    "\n",
    "def calculate_prediction_intervals(model, X, percentile=95):\n",
    "    \"\"\"Calculate prediction intervals using bootstrapping for ensemble models.\n",
    "    \n",
    "    Args:\n",
    "        model: Fitted pipeline with an ensemble model\n",
    "        X: Input features\n",
    "        percentile: The percentile range for the prediction interval (default: 95)\n",
    "    \n",
    "    Returns:\n",
    "        lower, upper: Lower and upper bounds of the prediction interval\n",
    "    \"\"\"\n",
    "    # Get predictions from all trees in the forest\n",
    "    predictions = []\n",
    "    forest = model.named_steps['regressor']\n",
    "    \n",
    "    # Apply preprocessing to input data\n",
    "    X_processed = model.named_steps['preprocessor'].transform(X)\n",
    "    \n",
    "    # Get predictions from each tree\n",
    "    for estimator in forest.estimators_:\n",
    "        pred = estimator.predict(X_processed)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Convert to numpy array for percentile calculation\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate prediction intervals\n",
    "    lower = np.percentile(predictions, (100 - percentile) / 2, axis=0)\n",
    "    upper = np.percentile(predictions, 100 - (100 - percentile) / 2, axis=0)\n",
    "    \n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da684e",
   "metadata": {},
   "source": [
    "\n",
    "Train and evaluate all models on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910b1f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline (Linear)...\n",
      "\n",
      "Baseline (Linear) Performance:\n",
      "train_rmse: 0.1204\n",
      "test_rmse: 0.1206\n",
      "train_mae: 0.0851\n",
      "test_mae: 0.0853\n",
      "train_r2: 0.1413\n",
      "test_r2: 0.1412\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Baseline (Linear) Performance:\n",
      "train_rmse: 0.1204\n",
      "test_rmse: 0.1206\n",
      "train_mae: 0.0851\n",
      "test_mae: 0.0853\n",
      "train_r2: 0.1413\n",
      "test_r2: 0.1412\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Random Forest Performance:\n",
      "train_rmse: 0.0676\n",
      "test_rmse: 0.0907\n",
      "train_mae: 0.0483\n",
      "test_mae: 0.0636\n",
      "train_r2: 0.7290\n",
      "test_r2: 0.5141\n",
      "\n",
      "Training Gradient Boosting...\n",
      "\n",
      "Random Forest Performance:\n",
      "train_rmse: 0.0676\n",
      "test_rmse: 0.0907\n",
      "train_mae: 0.0483\n",
      "test_mae: 0.0636\n",
      "train_r2: 0.7290\n",
      "test_r2: 0.5141\n",
      "\n",
      "Training Gradient Boosting...\n",
      "\n",
      "Gradient Boosting Performance:\n",
      "train_rmse: 0.1151\n",
      "test_rmse: 0.1156\n",
      "train_mae: 0.0819\n",
      "test_mae: 0.0823\n",
      "train_r2: 0.2148\n",
      "test_r2: 0.2108\n",
      "\n",
      "Training Voting Ensemble...\n",
      "\n",
      "Gradient Boosting Performance:\n",
      "train_rmse: 0.1151\n",
      "test_rmse: 0.1156\n",
      "train_mae: 0.0819\n",
      "test_mae: 0.0823\n",
      "train_r2: 0.2148\n",
      "test_r2: 0.2108\n",
      "\n",
      "Training Voting Ensemble...\n",
      "\n",
      "Voting Ensemble Performance:\n",
      "train_rmse: 0.0896\n",
      "test_rmse: 0.1012\n",
      "train_mae: 0.0645\n",
      "test_mae: 0.0723\n",
      "train_r2: 0.5246\n",
      "test_r2: 0.3956\n",
      "\n",
      "Model Comparison:\n",
      "            Baseline (Linear)  Random Forest  Gradient Boosting  \\\n",
      "train_rmse             0.1204         0.0676             0.1151   \n",
      "test_rmse              0.1206         0.0907             0.1156   \n",
      "train_mae              0.0851         0.0483             0.0819   \n",
      "test_mae               0.0853         0.0636             0.0823   \n",
      "train_r2               0.1413         0.7290             0.2148   \n",
      "test_r2                0.1412         0.5141             0.2108   \n",
      "\n",
      "            Voting Ensemble  \n",
      "train_rmse           0.0896  \n",
      "test_rmse            0.1012  \n",
      "train_mae            0.0645  \n",
      "test_mae             0.0723  \n",
      "train_r2             0.5246  \n",
      "test_r2              0.3956  \n",
      "\n",
      "Voting Ensemble Performance:\n",
      "train_rmse: 0.0896\n",
      "test_rmse: 0.1012\n",
      "train_mae: 0.0645\n",
      "test_mae: 0.0723\n",
      "train_r2: 0.5246\n",
      "test_r2: 0.3956\n",
      "\n",
      "Model Comparison:\n",
      "            Baseline (Linear)  Random Forest  Gradient Boosting  \\\n",
      "train_rmse             0.1204         0.0676             0.1151   \n",
      "test_rmse              0.1206         0.0907             0.1156   \n",
      "train_mae              0.0851         0.0483             0.0819   \n",
      "test_mae               0.0853         0.0636             0.0823   \n",
      "train_r2               0.1413         0.7290             0.2148   \n",
      "test_r2                0.1412         0.5141             0.2108   \n",
      "\n",
      "            Voting Ensemble  \n",
      "train_rmse           0.0896  \n",
      "test_rmse            0.1012  \n",
      "train_mae            0.0645  \n",
      "test_mae             0.0723  \n",
      "train_r2             0.5246  \n",
      "test_r2              0.3956  \n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Baseline (Linear)': baseline_pipeline,\n",
    "    'Random Forest': rf_pipeline,\n",
    "    'Gradient Boosting': gb_pipeline,\n",
    "    'Voting Ensemble': voting_pipeline\n",
    "}\n",
    "\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    metrics, train_pred, test_pred = evaluate_model(\n",
    "        model, X_train, X_test, y_train, y_test, name\n",
    "    )\n",
    "    results[name] = metrics\n",
    "    predictions[name] = {'train': train_pred, 'test': test_pred}\n",
    "\n",
    "# Convert results to DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f46ea9",
   "metadata": {},
   "source": [
    "\n",
    "Create comprehensive visualizations of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba107099",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for name, preds in predictions.items():\n",
    "    plt.scatter(y_test, preds['test'], alpha=0.5, label=name)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "plt.xlabel('Actual Risk Score')\n",
    "plt.ylabel('Predicted Risk Score')\n",
    "plt.title('Predicted vs Actual - All Models')\n",
    "plt.legend()\n",
    "plt.savefig(figures_dir / 'predicted_vs_actual.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Feature Importance Plots\n",
    "for name, model in models.items():\n",
    "    if hasattr(model.named_steps['regressor'], 'feature_importances_'):\n",
    "        fig = plot_feature_importance(model, selected_features, f\"{name} - Feature Importance\")\n",
    "        fig.savefig(figures_dir / f'feature_importance_{name.lower().replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "\n",
    "# 3. Residual Plots\n",
    "for name, preds in predictions.items():\n",
    "    fig = plot_residuals(y_test, preds['test'], f\"{name} - Residuals\")\n",
    "    fig.savefig(figures_dir / f'residuals_{name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "# 4. Temporal Performance\n",
    "temporal_results = {}\n",
    "for name, model in models.items():\n",
    "    temporal_metrics = temporal_performance(model, X_test, y_test, 'time_window')\n",
    "    temporal_results[name] = temporal_metrics\n",
    "\n",
    "# Plot temporal performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, metrics in temporal_results.items():\n",
    "    plt.plot(metrics['period'], metrics['r2'], label=name, marker='o')\n",
    "plt.xlabel('Time Window')\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.title('Model Performance Over Time')\n",
    "plt.legend()\n",
    "plt.savefig(figures_dir / 'temporal_performance.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34494ab1",
   "metadata": {},
   "source": [
    "\n",
    "Based on the comprehensive evaluation, select the best model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0926c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models\\\\best_model.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = rf_pipeline\n",
    "best_model_name = 'Random Forest'\n",
    "\n",
    "# Calculate prediction intervals for the best model\n",
    "if isinstance(best_model.named_steps['regressor'], RandomForestRegressor):\n",
    "    lower, upper = calculate_prediction_intervals(best_model, X_test)\n",
    "    prediction_intervals = pd.DataFrame({\n",
    "        'actual': y_test,\n",
    "        'predicted': predictions[best_model_name]['test'],\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    })\n",
    "    \n",
    "    # Plot predictions with intervals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, predictions[best_model_name]['test'], alpha=0.5)\n",
    "    plt.fill_between(y_test, lower, upper, alpha=0.2, label='95% Prediction Interval')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
    "    plt.xlabel('Actual Risk Score')\n",
    "    plt.ylabel('Predicted Risk Score')\n",
    "    plt.title(f'{best_model_name} Predictions with 95% Intervals')\n",
    "    plt.legend()\n",
    "    plt.savefig(figures_dir / 'best_model_predictions_with_intervals.png')\n",
    "    plt.close()\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(results_dir / 'final_model_results.csv')\n",
    "prediction_intervals.to_csv(results_dir / 'prediction_intervals.csv')\n",
    "\n",
    "# Save best model\n",
    "dump(best_model, models_dir / 'best_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
